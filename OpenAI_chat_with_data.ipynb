{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat with your data\n",
    "\n",
    "### This file is for educational purpose, it explains the flow from data to chat.\n",
    "\n",
    "### The easiest way to run this file is in Google Colab.\n",
    "### https://colab.research.google.com/\n",
    "### Log in with your Google-account and add this file in \"Ladda Upp\".\n",
    "\n",
    "### In Colab you can run all cells with Ctrl+F9 or press Shift+Enter to run chosen cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this cell we install our packages\n",
    "\n",
    "!pip install -qU \\\n",
    "    langchain \\\n",
    "    openai==0.28.0 \\\n",
    "    pinecone-client==2.2.4 \\\n",
    "    tiktoken==0.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "\n",
    "# Add your unique key here.\n",
    "os.environ[\"OPENAI_API_KEY\"] =\"ENTER-YOUR-OPENAI-KEY-HERE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you have the data in a excel-file you can use the read_csv function. We have removed the top line of the excel-file and and saved it as a csv before we insert the the path in the function.\n",
    "### All the columns are used as the data. We choose to do this so it would be more easier to use any machine data. All the missing values will be representated by 'null'. This is not necessary but it will help to ai if the data is consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# If you are running this in Google Colab you have to add the file to a folder. Press the folder icon on the left side of screen. Add the path to your file below.\n",
    "data = pd.read_csv(r\"data\\2830612_VMMS_EWO_KPI_clean.csv\", sep=\";\")\n",
    "\n",
    "\n",
    "data.replace({np.nan: 'null', 'NA': 'null', 'N/A': 'null', 'Missing': 'null'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the data. This code only shows the first five rows.\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have custom made our own Documentloader.\n",
    "\n",
    "from typing import Any, Iterator, List\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.document_loaders.base import BaseLoader\n",
    "\n",
    "\n",
    "class DataFrameLoader2(BaseLoader):\n",
    "    \"\"\"Load from a Pandas DataFrame.\"\"\"\n",
    "\n",
    "    def __init__(self, data_frame: Any, ewo_wo_no_column: str = \"EWO WO No\"):\n",
    "        \"\"\"Initialize with a Pandas DataFrame.\n",
    "\n",
    "        Args:\n",
    "            data_frame: Pandas DataFrame.\n",
    "            ewo_wo_no_column: Name of the column to include as metadata. Defaults to \"EWO WO No\".\n",
    "        \"\"\"\n",
    "        if not isinstance(data_frame, pd.DataFrame):\n",
    "            raise ValueError(\n",
    "                f\"Expected data_frame to be a pd.DataFrame, got {type(data_frame)}\"\n",
    "            )\n",
    "        self.data_frame = data_frame\n",
    "        self.ewo_wo_no_column = ewo_wo_no_column\n",
    "\n",
    "    def lazy_load(self) -> Iterator[Document]:\n",
    "        \"\"\"Lazy load records from the Pandas DataFrame.\"\"\"\n",
    "\n",
    "        for _, row in self.data_frame.iterrows():\n",
    "            text = \"\\n\".join([f\"{key}: {value}\" for key, value in row.items()])\n",
    "            metadata = row.to_dict()\n",
    "            ewo_wo_no_value = row[self.ewo_wo_no_column]\n",
    "            metadata[\"EWO WO No\"] = ewo_wo_no_value\n",
    "            yield Document(page_content=text, metadata=metadata, column_names=list(self.data_frame.columns))\n",
    "\n",
    "    def load(self) -> List[Document]:\n",
    "        \"\"\"Load full Pandas DataFrame.\"\"\"\n",
    "        return list(self.lazy_load())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There ara many types of loaders that create the data into a Document. A Document consist of page_content and metadata\n",
    "### In the different types of loaders you can specify which data/column is page_content and which is metadata.\n",
    "\n",
    "### Read all about it here https://python.langchain.com/docs/modules/data_connection/document_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loader = DataFrameLoader2(data)\n",
    "\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]\n",
    "\n",
    "# Show the first Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0].metadata\n",
    "\n",
    "# Run this cell to show the metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of the csv you can read in the data as a json-file. Add the file to your folder and change the file path in the code.\n",
    "\n",
    "json = pd.read_json(r\"C:\\Users\\debbi\\NLP\\vol.json\", orient=True)\n",
    "\n",
    "loader = DataFrameLoader2(json)\n",
    "data = loader.load()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings and storing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we assign the FAISS vector index to be the chats database. \n",
    "# FAISS (Facebook AI simularity search) is Meta library for handeling similarity searches..\n",
    "db = FAISS.from_documents(data, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can query your database.\n",
    "\n",
    "query = db.similarity_search(\"VÃ¥g 1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The top most simulary documents to your query\n",
    "query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    model='gpt-4'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can change the prompt but the {question} and {context} is key values that the models needs. Removing these will cause an error.\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"I will provide you with historical data regarding previous work orders (EWO) for a machine. By examining the historical data, you should respond by describing similar issues in the historical data and how those issues were resolved. Use all the columns and explain how a similar problem was previously resolved.\n",
    "\n",
    "Before you do that, ask for a brief description of the issue that we will analyze using the historical data you've received.\n",
    "\n",
    "This description may be concise and may not exactly match any previous ones in the history. Nevertheless, you should search the history for similar problems or solutions resembling the new issue description.\n",
    "\n",
    "Especially, check what's mentioned in EWO WO Directive, EWO WO Work Done, and EWO WO Work Details. Also, think broadly because there might be misspellings or similar words with the same meaning.\n",
    "\n",
    "Reply with the number of work orders available. Also, mention how many of them are similar to the issue we're currently analyzing.\n",
    "\n",
    "Then, proceed with details about all of them. This includes all analyses you can perform about what was done, the time taken, parts replaced, downtime, etc. Time analyses are crucial. Specify the time taken to resolve the issue, the duration from discovery to operation, and, most importantly, thoroughly analyze the root cause. Include all time-related information.\n",
    "\n",
    "If you encounter any issues while reading the file, do not mention it, just continue. Do not describe all the steps you take. The information you are getting is EWO and you will analyzie it with as much information as needed. The EWO WO ID is the link between the texts. If I ask you for date or time, this is the correct format to look for: YYYY-MM-DD HH:MI:SS. Give the answer about the date if someone asks. If you don't know the answer, just say that you don't know, don't try to make up an answer. If you encounter  some technical terms may not have direct translations in English write the whole answer in english.\n",
    "If you are looking for a datetime the format to look for is YYYY:MM:DD HH:MI:SS. Don't answer if the chat_history is empty.\n",
    "{context}\n",
    "\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer in Swedish:\"\"\"\n",
    "\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=template, input_variables=[\"question\", \"context\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you want the chat to remember the conversation you need a memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time for the final step - putting all the pieces together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "gpt_pipeline = RetrievalQA.from_chain_type(\n",
    "    llm=chat, chain_type='stuff',\n",
    "    chain_type_kwargs = {\"prompt\": PROMPT},\n",
    "    memory = memory,\n",
    "    retriever=db.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_pipeline(\"Hej\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If you print the pipeline you can see all the information about the object.\n",
    "print(gpt_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_pipeline(\"Kan du hjÃ¤lpa mig hitta ett datum fÃ¶r nÃ¤r vi senast hade problem med sandvagnar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_pipeline(\"Kan du hÃ¤mta Kaizen Ã¥tgÃ¤rd fÃ¶r detta?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gpt_pipeline(\"Har detta hÃ¤nt tidigare? Hur lÃ¶stes det dÃ¥?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_pipeline(\"Hur lÃ¥ngt tid tog det att lÃ¶sa problem med VÃ¥g 1 vid varje fall?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_pipeline(\"Finns det en Kaizen Ã¥tgÃ¤rds gjort pÃ¥ fall rÃ¶rande vÃ¥g 1?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_pipeline(\"vilken tidpunkt lÃ¶stes det?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envNLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
